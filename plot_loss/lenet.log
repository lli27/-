Log file created at: 2017/06/25 20:55:02
Running on machine: LILY
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0625 20:55:02.795395 12648 caffe.cpp:179] Use CPU.
I0625 20:55:02.795395 12648 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_test.prototxt"
I0625 20:55:02.811044 12648 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0625 20:55:02.811044 12648 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0625 20:55:02.811044 12648 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0625 20:55:02.811044 12648 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0625 20:55:02.811044 12648 layer_factory.hpp:77] Creating layer mnist
I0625 20:55:02.811044 12648 common.cpp:37] System entropy source not available, using fallback algorithm to generate seed instead.
I0625 20:55:02.811044 12648 net.cpp:91] Creating Layer mnist
I0625 20:55:02.811044 12648 net.cpp:399] mnist -> data
I0625 20:55:02.811044 10476 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_train_lmdb
I0625 20:55:02.811044 12648 net.cpp:399] mnist -> label
I0625 20:55:02.811044 12648 data_layer.cpp:41] output data size: 64,1,28,28
I0625 20:55:02.826612 12648 net.cpp:141] Setting up mnist
I0625 20:55:02.826612 12648 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0625 20:55:02.826612 12648 net.cpp:148] Top shape: 64 (64)
I0625 20:55:02.826612 12648 net.cpp:156] Memory required for data: 200960
I0625 20:55:02.826612 12648 layer_factory.hpp:77] Creating layer conv1
I0625 20:55:02.826612 12648 net.cpp:91] Creating Layer conv1
I0625 20:55:02.826612 12648 net.cpp:425] conv1 <- data
I0625 20:55:02.826612 12648 net.cpp:399] conv1 -> conv1
I0625 20:55:02.826612 12648 net.cpp:141] Setting up conv1
I0625 20:55:02.826612 12648 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0625 20:55:02.826612 12648 net.cpp:156] Memory required for data: 3150080
I0625 20:55:02.826612 12648 layer_factory.hpp:77] Creating layer pool1
I0625 20:55:02.826612 12648 net.cpp:91] Creating Layer pool1
I0625 20:55:02.826612 12648 net.cpp:425] pool1 <- conv1
I0625 20:55:02.826612 12648 net.cpp:399] pool1 -> pool1
I0625 20:55:02.826612 12648 net.cpp:141] Setting up pool1
I0625 20:55:02.826612 12648 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0625 20:55:02.826612 12648 net.cpp:156] Memory required for data: 3887360
I0625 20:55:02.826612 12648 layer_factory.hpp:77] Creating layer conv2
I0625 20:55:02.826612 12648 net.cpp:91] Creating Layer conv2
I0625 20:55:02.826612 12648 net.cpp:425] conv2 <- pool1
I0625 20:55:02.826612 12648 net.cpp:399] conv2 -> conv2
I0625 20:55:02.842241 12648 net.cpp:141] Setting up conv2
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 4706560
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer pool2
I0625 20:55:02.842241 12648 net.cpp:91] Creating Layer pool2
I0625 20:55:02.842241 12648 net.cpp:425] pool2 <- conv2
I0625 20:55:02.842241 12648 net.cpp:399] pool2 -> pool2
I0625 20:55:02.842241 12648 net.cpp:141] Setting up pool2
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 4911360
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer ip1
I0625 20:55:02.842241 12648 net.cpp:91] Creating Layer ip1
I0625 20:55:02.842241 12648 net.cpp:425] ip1 <- pool2
I0625 20:55:02.842241 12648 net.cpp:399] ip1 -> ip1
I0625 20:55:02.842241 12648 net.cpp:141] Setting up ip1
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: 64 500 (32000)
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 5039360
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer relu1
I0625 20:55:02.842241 12648 net.cpp:91] Creating Layer relu1
I0625 20:55:02.842241 12648 net.cpp:425] relu1 <- ip1
I0625 20:55:02.842241 12648 net.cpp:386] relu1 -> ip1 (in-place)
I0625 20:55:02.842241 12648 net.cpp:141] Setting up relu1
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: 64 500 (32000)
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 5167360
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer ip2
I0625 20:55:02.842241 12648 net.cpp:91] Creating Layer ip2
I0625 20:55:02.842241 12648 net.cpp:425] ip2 <- ip1
I0625 20:55:02.842241 12648 net.cpp:399] ip2 -> ip2
I0625 20:55:02.842241 12648 net.cpp:141] Setting up ip2
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: 64 10 (640)
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 5169920
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer loss
I0625 20:55:02.842241 12648 net.cpp:91] Creating Layer loss
I0625 20:55:02.842241 12648 net.cpp:425] loss <- ip2
I0625 20:55:02.842241 12648 net.cpp:425] loss <- label
I0625 20:55:02.842241 12648 net.cpp:399] loss -> loss
I0625 20:55:02.842241 12648 layer_factory.hpp:77] Creating layer loss
I0625 20:55:02.842241 12648 net.cpp:141] Setting up loss
I0625 20:55:02.842241 12648 net.cpp:148] Top shape: (1)
I0625 20:55:02.842241 12648 net.cpp:151]     with loss weight 1
I0625 20:55:02.842241 12648 net.cpp:156] Memory required for data: 5169924
I0625 20:55:02.842241 12648 net.cpp:217] loss needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] ip2 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] relu1 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] ip1 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] pool2 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] conv2 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] pool1 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:217] conv1 needs backward computation.
I0625 20:55:02.842241 12648 net.cpp:219] mnist does not need backward computation.
I0625 20:55:02.842241 12648 net.cpp:261] This network produces output loss
I0625 20:55:02.842241 12648 net.cpp:274] Network initialization done.
I0625 20:55:02.842241 12648 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0625 20:55:02.842241 12648 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0625 20:55:02.842241 12648 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer mnist
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer mnist
I0625 20:55:02.857864 12648 net.cpp:399] mnist -> data
I0625 20:55:02.857864 12648 net.cpp:399] mnist -> label
I0625 20:55:02.857864 15824 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_test_lmdb
I0625 20:55:02.857864 12648 data_layer.cpp:41] output data size: 100,1,28,28
I0625 20:55:02.857864 12648 net.cpp:141] Setting up mnist
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 (100)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 314000
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer label_mnist_1_split
I0625 20:55:02.857864 12648 net.cpp:425] label_mnist_1_split <- label
I0625 20:55:02.857864 12648 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0625 20:55:02.857864 12648 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0625 20:55:02.857864 12648 net.cpp:141] Setting up label_mnist_1_split
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 (100)
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 (100)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 314800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer conv1
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer conv1
I0625 20:55:02.857864 12648 net.cpp:425] conv1 <- data
I0625 20:55:02.857864 12648 net.cpp:399] conv1 -> conv1
I0625 20:55:02.857864 12648 net.cpp:141] Setting up conv1
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 4922800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer pool1
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer pool1
I0625 20:55:02.857864 12648 net.cpp:425] pool1 <- conv1
I0625 20:55:02.857864 12648 net.cpp:399] pool1 -> pool1
I0625 20:55:02.857864 12648 net.cpp:141] Setting up pool1
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 6074800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer conv2
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer conv2
I0625 20:55:02.857864 12648 net.cpp:425] conv2 <- pool1
I0625 20:55:02.857864 12648 net.cpp:399] conv2 -> conv2
I0625 20:55:02.857864 12648 net.cpp:141] Setting up conv2
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 7354800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer pool2
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer pool2
I0625 20:55:02.857864 12648 net.cpp:425] pool2 <- conv2
I0625 20:55:02.857864 12648 net.cpp:399] pool2 -> pool2
I0625 20:55:02.857864 12648 net.cpp:141] Setting up pool2
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 7674800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer ip1
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer ip1
I0625 20:55:02.857864 12648 net.cpp:425] ip1 <- pool2
I0625 20:55:02.857864 12648 net.cpp:399] ip1 -> ip1
I0625 20:55:02.857864 12648 net.cpp:141] Setting up ip1
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 500 (50000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 7874800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer relu1
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer relu1
I0625 20:55:02.857864 12648 net.cpp:425] relu1 <- ip1
I0625 20:55:02.857864 12648 net.cpp:386] relu1 -> ip1 (in-place)
I0625 20:55:02.857864 12648 net.cpp:141] Setting up relu1
I0625 20:55:02.857864 12648 net.cpp:148] Top shape: 100 500 (50000)
I0625 20:55:02.857864 12648 net.cpp:156] Memory required for data: 8074800
I0625 20:55:02.857864 12648 layer_factory.hpp:77] Creating layer ip2
I0625 20:55:02.857864 12648 net.cpp:91] Creating Layer ip2
I0625 20:55:02.857864 12648 net.cpp:425] ip2 <- ip1
I0625 20:55:02.857864 12648 net.cpp:399] ip2 -> ip2
I0625 20:55:02.873492 12648 net.cpp:141] Setting up ip2
I0625 20:55:02.873492 12648 net.cpp:148] Top shape: 100 10 (1000)
I0625 20:55:02.873492 12648 net.cpp:156] Memory required for data: 8078800
I0625 20:55:02.873492 12648 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0625 20:55:02.873492 12648 net.cpp:91] Creating Layer ip2_ip2_0_split
I0625 20:55:02.873492 12648 net.cpp:425] ip2_ip2_0_split <- ip2
I0625 20:55:02.873492 12648 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0625 20:55:02.873492 12648 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0625 20:55:02.873492 12648 net.cpp:141] Setting up ip2_ip2_0_split
I0625 20:55:02.873492 12648 net.cpp:148] Top shape: 100 10 (1000)
I0625 20:55:02.873492 12648 net.cpp:148] Top shape: 100 10 (1000)
I0625 20:55:02.873492 12648 net.cpp:156] Memory required for data: 8086800
I0625 20:55:02.873492 12648 layer_factory.hpp:77] Creating layer accuracy
I0625 20:55:02.873492 12648 net.cpp:91] Creating Layer accuracy
I0625 20:55:02.873492 12648 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0625 20:55:02.873492 12648 net.cpp:425] accuracy <- label_mnist_1_split_0
I0625 20:55:02.873492 12648 net.cpp:399] accuracy -> accuracy
I0625 20:55:02.873492 12648 net.cpp:141] Setting up accuracy
I0625 20:55:02.889117 12648 net.cpp:148] Top shape: (1)
I0625 20:55:02.889117 12648 net.cpp:156] Memory required for data: 8086804
I0625 20:55:02.889117 12648 layer_factory.hpp:77] Creating layer loss
I0625 20:55:02.889117 12648 net.cpp:91] Creating Layer loss
I0625 20:55:02.889117 12648 net.cpp:425] loss <- ip2_ip2_0_split_1
I0625 20:55:02.889117 12648 net.cpp:425] loss <- label_mnist_1_split_1
I0625 20:55:02.889117 12648 net.cpp:399] loss -> loss
I0625 20:55:02.889117 12648 layer_factory.hpp:77] Creating layer loss
I0625 20:55:02.889117 12648 net.cpp:141] Setting up loss
I0625 20:55:02.889117 12648 net.cpp:148] Top shape: (1)
I0625 20:55:02.889117 12648 net.cpp:151]     with loss weight 1
I0625 20:55:02.889117 12648 net.cpp:156] Memory required for data: 8086808
I0625 20:55:02.889117 12648 net.cpp:217] loss needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:219] accuracy does not need backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] ip2 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] relu1 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] ip1 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] pool2 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] conv2 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] pool1 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:217] conv1 needs backward computation.
I0625 20:55:02.889117 12648 net.cpp:219] label_mnist_1_split does not need backward computation.
I0625 20:55:02.889117 12648 net.cpp:219] mnist does not need backward computation.
I0625 20:55:02.889117 12648 net.cpp:261] This network produces output accuracy
I0625 20:55:02.889117 12648 net.cpp:261] This network produces output loss
I0625 20:55:02.889117 12648 net.cpp:274] Network initialization done.
I0625 20:55:02.889117 12648 solver.cpp:60] Solver scaffolding done.
I0625 20:55:02.889117 12648 caffe.cpp:220] Starting Optimization
I0625 20:55:02.889117 12648 solver.cpp:279] Solving LeNet
I0625 20:55:02.889117 12648 solver.cpp:280] Learning Rate Policy: inv
I0625 20:55:02.889117 12648 solver.cpp:337] Iteration 0, Testing net (#0)
I0625 20:55:06.420748 12648 solver.cpp:404]     Test net output #0: accuracy = 0.0847
I0625 20:55:06.420748 12648 solver.cpp:404]     Test net output #1: loss = 2.31585 (* 1 = 2.31585 loss)
I0625 20:55:06.467628 12648 solver.cpp:228] Iteration 0, loss = 2.29362
I0625 20:55:06.467628 12648 solver.cpp:244]     Train net output #0: loss = 2.29362 (* 1 = 2.29362 loss)
I0625 20:55:06.467628 12648 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0625 20:55:11.671313 12648 solver.cpp:228] Iteration 100, loss = 0.226025
I0625 20:55:11.671313 12648 solver.cpp:244]     Train net output #0: loss = 0.226025 (* 1 = 0.226025 loss)
I0625 20:55:11.671313 12648 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0625 20:55:16.859371 12648 solver.cpp:228] Iteration 200, loss = 0.15187
I0625 20:55:16.859371 12648 solver.cpp:244]     Train net output #0: loss = 0.15187 (* 1 = 0.15187 loss)
I0625 20:55:16.859371 12648 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0625 20:55:22.047428 12648 solver.cpp:228] Iteration 300, loss = 0.165072
I0625 20:55:22.047428 12648 solver.cpp:244]     Train net output #0: loss = 0.165072 (* 1 = 0.165072 loss)
I0625 20:55:22.047428 12648 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0625 20:55:27.235487 12648 solver.cpp:228] Iteration 400, loss = 0.0587308
I0625 20:55:27.235487 12648 solver.cpp:244]     Train net output #0: loss = 0.0587309 (* 1 = 0.0587309 loss)
I0625 20:55:27.235487 12648 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0625 20:55:32.345412 12648 solver.cpp:337] Iteration 500, Testing net (#0)
I0625 20:55:35.814535 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9723
I0625 20:55:35.814535 12648 solver.cpp:404]     Test net output #1: loss = 0.0878518 (* 1 = 0.0878518 loss)
I0625 20:55:35.877041 12648 solver.cpp:228] Iteration 500, loss = 0.103714
I0625 20:55:35.877041 12648 solver.cpp:244]     Train net output #0: loss = 0.103714 (* 1 = 0.103714 loss)
I0625 20:55:35.877041 12648 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0625 20:55:41.205740 12648 solver.cpp:228] Iteration 600, loss = 0.0742457
I0625 20:55:41.205740 12648 solver.cpp:244]     Train net output #0: loss = 0.0742458 (* 1 = 0.0742458 loss)
I0625 20:55:41.205740 12648 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0625 20:55:46.362545 12648 solver.cpp:228] Iteration 700, loss = 0.148119
I0625 20:55:46.362545 12648 solver.cpp:244]     Train net output #0: loss = 0.148119 (* 1 = 0.148119 loss)
I0625 20:55:46.362545 12648 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0625 20:55:51.534976 12648 solver.cpp:228] Iteration 800, loss = 0.230655
I0625 20:55:51.534976 12648 solver.cpp:244]     Train net output #0: loss = 0.230655 (* 1 = 0.230655 loss)
I0625 20:55:51.534976 12648 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0625 20:55:56.707408 12648 solver.cpp:228] Iteration 900, loss = 0.166655
I0625 20:55:56.707408 12648 solver.cpp:244]     Train net output #0: loss = 0.166655 (* 1 = 0.166655 loss)
I0625 20:55:56.707408 12648 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0625 20:56:01.832960 12648 solver.cpp:337] Iteration 1000, Testing net (#0)
I0625 20:56:05.286456 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9825
I0625 20:56:05.286456 12648 solver.cpp:404]     Test net output #1: loss = 0.0563498 (* 1 = 0.0563498 loss)
I0625 20:56:05.333336 12648 solver.cpp:228] Iteration 1000, loss = 0.0699653
I0625 20:56:05.333336 12648 solver.cpp:244]     Train net output #0: loss = 0.0699653 (* 1 = 0.0699653 loss)
I0625 20:56:05.333336 12648 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0625 20:56:10.537021 12648 solver.cpp:228] Iteration 1100, loss = 0.00456115
I0625 20:56:10.537021 12648 solver.cpp:244]     Train net output #0: loss = 0.00456115 (* 1 = 0.00456115 loss)
I0625 20:56:10.537021 12648 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0625 20:56:15.709453 12648 solver.cpp:228] Iteration 1200, loss = 0.0190241
I0625 20:56:15.709453 12648 solver.cpp:244]     Train net output #0: loss = 0.0190241 (* 1 = 0.0190241 loss)
I0625 20:56:15.709453 12648 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0625 20:56:20.913137 12648 solver.cpp:228] Iteration 1300, loss = 0.0322443
I0625 20:56:20.913137 12648 solver.cpp:244]     Train net output #0: loss = 0.0322443 (* 1 = 0.0322443 loss)
I0625 20:56:20.913137 12648 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0625 20:56:26.069942 12648 solver.cpp:228] Iteration 1400, loss = 0.00776143
I0625 20:56:26.069942 12648 solver.cpp:244]     Train net output #0: loss = 0.00776148 (* 1 = 0.00776148 loss)
I0625 20:56:26.069942 12648 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0625 20:56:31.195494 12648 solver.cpp:337] Iteration 1500, Testing net (#0)
I0625 20:56:34.648990 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9846
I0625 20:56:34.648990 12648 solver.cpp:404]     Test net output #1: loss = 0.0482743 (* 1 = 0.0482743 loss)
I0625 20:56:34.695870 12648 solver.cpp:228] Iteration 1500, loss = 0.0738846
I0625 20:56:34.695870 12648 solver.cpp:244]     Train net output #0: loss = 0.0738846 (* 1 = 0.0738846 loss)
I0625 20:56:34.695870 12648 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0625 20:56:39.868301 12648 solver.cpp:228] Iteration 1600, loss = 0.100623
I0625 20:56:39.868301 12648 solver.cpp:244]     Train net output #0: loss = 0.100623 (* 1 = 0.100623 loss)
I0625 20:56:39.868301 12648 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0625 20:56:45.040735 12648 solver.cpp:228] Iteration 1700, loss = 0.0292663
I0625 20:56:45.040735 12648 solver.cpp:244]     Train net output #0: loss = 0.0292663 (* 1 = 0.0292663 loss)
I0625 20:56:45.040735 12648 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0625 20:56:50.228791 12648 solver.cpp:228] Iteration 1800, loss = 0.0249491
I0625 20:56:50.228791 12648 solver.cpp:244]     Train net output #0: loss = 0.0249491 (* 1 = 0.0249491 loss)
I0625 20:56:50.228791 12648 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0625 20:56:55.385597 12648 solver.cpp:228] Iteration 1900, loss = 0.131964
I0625 20:56:55.385597 12648 solver.cpp:244]     Train net output #0: loss = 0.131964 (* 1 = 0.131964 loss)
I0625 20:56:55.385597 12648 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0625 20:57:00.511148 12648 solver.cpp:337] Iteration 2000, Testing net (#0)
I0625 20:57:03.980273 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9864
I0625 20:57:03.980273 12648 solver.cpp:404]     Test net output #1: loss = 0.0419966 (* 1 = 0.0419966 loss)
I0625 20:57:04.027151 12648 solver.cpp:228] Iteration 2000, loss = 0.0159766
I0625 20:57:04.027151 12648 solver.cpp:244]     Train net output #0: loss = 0.0159766 (* 1 = 0.0159766 loss)
I0625 20:57:04.027151 12648 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0625 20:57:09.199584 12648 solver.cpp:228] Iteration 2100, loss = 0.0187814
I0625 20:57:09.199584 12648 solver.cpp:244]     Train net output #0: loss = 0.0187814 (* 1 = 0.0187814 loss)
I0625 20:57:09.199584 12648 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0625 20:57:14.372014 12648 solver.cpp:228] Iteration 2200, loss = 0.0124329
I0625 20:57:14.372014 12648 solver.cpp:244]     Train net output #0: loss = 0.0124329 (* 1 = 0.0124329 loss)
I0625 20:57:14.372014 12648 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0625 20:57:19.638206 12648 solver.cpp:228] Iteration 2300, loss = 0.09941
I0625 20:57:19.638206 12648 solver.cpp:244]     Train net output #0: loss = 0.09941 (* 1 = 0.09941 loss)
I0625 20:57:19.638206 12648 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0625 20:57:24.810638 12648 solver.cpp:228] Iteration 2400, loss = 0.0145365
I0625 20:57:24.810638 12648 solver.cpp:244]     Train net output #0: loss = 0.0145366 (* 1 = 0.0145366 loss)
I0625 20:57:24.810638 12648 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0625 20:57:29.951817 12648 solver.cpp:337] Iteration 2500, Testing net (#0)
I0625 20:57:33.389686 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9851
I0625 20:57:33.389686 12648 solver.cpp:404]     Test net output #1: loss = 0.0482769 (* 1 = 0.0482769 loss)
I0625 20:57:33.436566 12648 solver.cpp:228] Iteration 2500, loss = 0.0301279
I0625 20:57:33.436566 12648 solver.cpp:244]     Train net output #0: loss = 0.0301279 (* 1 = 0.0301279 loss)
I0625 20:57:33.452193 12648 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0625 20:57:38.608997 12648 solver.cpp:228] Iteration 2600, loss = 0.0861182
I0625 20:57:38.608997 12648 solver.cpp:244]     Train net output #0: loss = 0.0861183 (* 1 = 0.0861183 loss)
I0625 20:57:38.608997 12648 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0625 20:57:43.797056 12648 solver.cpp:228] Iteration 2700, loss = 0.0426382
I0625 20:57:43.797056 12648 solver.cpp:244]     Train net output #0: loss = 0.0426382 (* 1 = 0.0426382 loss)
I0625 20:57:43.797056 12648 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0625 20:57:48.953860 12648 solver.cpp:228] Iteration 2800, loss = 0.00362801
I0625 20:57:48.953860 12648 solver.cpp:244]     Train net output #0: loss = 0.00362803 (* 1 = 0.00362803 loss)
I0625 20:57:48.953860 12648 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0625 20:57:54.126292 12648 solver.cpp:228] Iteration 2900, loss = 0.0130541
I0625 20:57:54.126292 12648 solver.cpp:244]     Train net output #0: loss = 0.0130541 (* 1 = 0.0130541 loss)
I0625 20:57:54.126292 12648 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0625 20:57:59.251843 12648 solver.cpp:337] Iteration 3000, Testing net (#0)
I0625 20:58:02.705340 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9873
I0625 20:58:02.705340 12648 solver.cpp:404]     Test net output #1: loss = 0.0396279 (* 1 = 0.0396279 loss)
I0625 20:58:02.752220 12648 solver.cpp:228] Iteration 3000, loss = 0.0115645
I0625 20:58:02.752220 12648 solver.cpp:244]     Train net output #0: loss = 0.0115646 (* 1 = 0.0115646 loss)
I0625 20:58:02.752220 12648 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0625 20:58:07.924651 12648 solver.cpp:228] Iteration 3100, loss = 0.0119264
I0625 20:58:07.924651 12648 solver.cpp:244]     Train net output #0: loss = 0.0119264 (* 1 = 0.0119264 loss)
I0625 20:58:07.924651 12648 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0625 20:58:13.190843 12648 solver.cpp:228] Iteration 3200, loss = 0.00416682
I0625 20:58:13.206470 12648 solver.cpp:244]     Train net output #0: loss = 0.00416686 (* 1 = 0.00416686 loss)
I0625 20:58:13.206470 12648 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0625 20:58:18.378902 12648 solver.cpp:228] Iteration 3300, loss = 0.0252897
I0625 20:58:18.378902 12648 solver.cpp:244]     Train net output #0: loss = 0.0252897 (* 1 = 0.0252897 loss)
I0625 20:58:18.378902 12648 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0625 20:58:23.551333 12648 solver.cpp:228] Iteration 3400, loss = 0.00805612
I0625 20:58:23.551333 12648 solver.cpp:244]     Train net output #0: loss = 0.00805614 (* 1 = 0.00805614 loss)
I0625 20:58:23.551333 12648 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0625 20:58:28.661258 12648 solver.cpp:337] Iteration 3500, Testing net (#0)
I0625 20:58:32.114755 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9857
I0625 20:58:32.114755 12648 solver.cpp:404]     Test net output #1: loss = 0.0437528 (* 1 = 0.0437528 loss)
I0625 20:58:32.161634 12648 solver.cpp:228] Iteration 3500, loss = 0.00514052
I0625 20:58:32.161634 12648 solver.cpp:244]     Train net output #0: loss = 0.00514052 (* 1 = 0.00514052 loss)
I0625 20:58:32.161634 12648 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0625 20:58:37.318439 12648 solver.cpp:228] Iteration 3600, loss = 0.0393647
I0625 20:58:37.318439 12648 solver.cpp:244]     Train net output #0: loss = 0.0393647 (* 1 = 0.0393647 loss)
I0625 20:58:37.318439 12648 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0625 20:58:42.506498 12648 solver.cpp:228] Iteration 3700, loss = 0.017183
I0625 20:58:42.506498 12648 solver.cpp:244]     Train net output #0: loss = 0.017183 (* 1 = 0.017183 loss)
I0625 20:58:42.506498 12648 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0625 20:58:47.678930 12648 solver.cpp:228] Iteration 3800, loss = 0.01217
I0625 20:58:47.678930 12648 solver.cpp:244]     Train net output #0: loss = 0.01217 (* 1 = 0.01217 loss)
I0625 20:58:47.678930 12648 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0625 20:58:53.038882 12648 solver.cpp:228] Iteration 3900, loss = 0.0431962
I0625 20:58:53.038882 12648 solver.cpp:244]     Train net output #0: loss = 0.0431962 (* 1 = 0.0431962 loss)
I0625 20:58:53.038882 12648 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0625 20:58:58.195685 12648 solver.cpp:337] Iteration 4000, Testing net (#0)
I0625 20:59:01.742944 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9893
I0625 20:59:01.742944 12648 solver.cpp:404]     Test net output #1: loss = 0.0310976 (* 1 = 0.0310976 loss)
I0625 20:59:01.789824 12648 solver.cpp:228] Iteration 4000, loss = 0.0233233
I0625 20:59:01.789824 12648 solver.cpp:244]     Train net output #0: loss = 0.0233233 (* 1 = 0.0233233 loss)
I0625 20:59:01.789824 12648 sgd_solver.cpp:106] Iteration 4000, lr = 0.00776969
I0625 20:59:07.556068 12648 solver.cpp:228] Iteration 4100, loss = 0.0409206
I0625 20:59:07.556068 12648 solver.cpp:244]     Train net output #0: loss = 0.0409206 (* 1 = 0.0409206 loss)
I0625 20:59:07.556068 12648 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0625 20:59:12.947273 12648 solver.cpp:228] Iteration 4200, loss = 0.00954479
I0625 20:59:12.947273 12648 solver.cpp:244]     Train net output #0: loss = 0.00954483 (* 1 = 0.00954483 loss)
I0625 20:59:12.962900 12648 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0625 20:59:18.150959 12648 solver.cpp:228] Iteration 4300, loss = 0.0325458
I0625 20:59:18.150959 12648 solver.cpp:244]     Train net output #0: loss = 0.0325459 (* 1 = 0.0325459 loss)
I0625 20:59:18.150959 12648 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0625 20:59:23.339016 12648 solver.cpp:228] Iteration 4400, loss = 0.0136276
I0625 20:59:23.339016 12648 solver.cpp:244]     Train net output #0: loss = 0.0136276 (* 1 = 0.0136276 loss)
I0625 20:59:23.339016 12648 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0625 20:59:28.511448 12648 solver.cpp:337] Iteration 4500, Testing net (#0)
I0625 20:59:31.949317 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9888
I0625 20:59:31.949317 12648 solver.cpp:404]     Test net output #1: loss = 0.0339398 (* 1 = 0.0339398 loss)
I0625 20:59:32.011832 12648 solver.cpp:228] Iteration 4500, loss = 0.00461188
I0625 20:59:32.011832 12648 solver.cpp:244]     Train net output #0: loss = 0.00461192 (* 1 = 0.00461192 loss)
I0625 20:59:32.011832 12648 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0625 20:59:37.168629 12648 solver.cpp:228] Iteration 4600, loss = 0.0124746
I0625 20:59:37.168629 12648 solver.cpp:244]     Train net output #0: loss = 0.0124746 (* 1 = 0.0124746 loss)
I0625 20:59:37.168629 12648 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0625 20:59:42.356688 12648 solver.cpp:228] Iteration 4700, loss = 0.0085325
I0625 20:59:42.356688 12648 solver.cpp:244]     Train net output #0: loss = 0.00853253 (* 1 = 0.00853253 loss)
I0625 20:59:42.356688 12648 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0625 20:59:47.513492 12648 solver.cpp:228] Iteration 4800, loss = 0.013702
I0625 20:59:47.513492 12648 solver.cpp:244]     Train net output #0: loss = 0.013702 (* 1 = 0.013702 loss)
I0625 20:59:47.529119 12648 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0625 20:59:52.685925 12648 solver.cpp:228] Iteration 4900, loss = 0.00738703
I0625 20:59:52.685925 12648 solver.cpp:244]     Train net output #0: loss = 0.00738705 (* 1 = 0.00738705 loss)
I0625 20:59:52.685925 12648 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0625 20:59:57.795850 12648 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0625 20:59:57.811475 12648 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0625 20:59:57.811475 12648 solver.cpp:337] Iteration 5000, Testing net (#0)
I0625 21:00:01.280598 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9909
I0625 21:00:01.280598 12648 solver.cpp:404]     Test net output #1: loss = 0.0300409 (* 1 = 0.0300409 loss)
I0625 21:00:01.343106 12648 solver.cpp:228] Iteration 5000, loss = 0.0345406
I0625 21:00:01.343106 12648 solver.cpp:244]     Train net output #0: loss = 0.0345407 (* 1 = 0.0345407 loss)
I0625 21:00:01.343106 12648 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0625 21:00:06.578044 12648 solver.cpp:228] Iteration 5100, loss = 0.0232882
I0625 21:00:06.578044 12648 solver.cpp:244]     Train net output #0: loss = 0.0232883 (* 1 = 0.0232883 loss)
I0625 21:00:06.578044 12648 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I0625 21:00:12.125516 12648 solver.cpp:228] Iteration 5200, loss = 0.0081957
I0625 21:00:12.125516 12648 solver.cpp:244]     Train net output #0: loss = 0.00819573 (* 1 = 0.00819573 loss)
I0625 21:00:12.125516 12648 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I0625 21:00:17.407335 12648 solver.cpp:228] Iteration 5300, loss = 0.000808873
I0625 21:00:17.407335 12648 solver.cpp:244]     Train net output #0: loss = 0.000808925 (* 1 = 0.000808925 loss)
I0625 21:00:17.407335 12648 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I0625 21:00:22.689152 12648 solver.cpp:228] Iteration 5400, loss = 0.0145583
I0625 21:00:22.689152 12648 solver.cpp:244]     Train net output #0: loss = 0.0145584 (* 1 = 0.0145584 loss)
I0625 21:00:22.689152 12648 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I0625 21:00:27.924094 12648 solver.cpp:337] Iteration 5500, Testing net (#0)
I0625 21:00:31.393214 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9895
I0625 21:00:31.393214 12648 solver.cpp:404]     Test net output #1: loss = 0.0315647 (* 1 = 0.0315647 loss)
I0625 21:00:31.455720 12648 solver.cpp:228] Iteration 5500, loss = 0.00746275
I0625 21:00:31.455720 12648 solver.cpp:244]     Train net output #0: loss = 0.00746278 (* 1 = 0.00746278 loss)
I0625 21:00:31.455720 12648 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I0625 21:00:36.721912 12648 solver.cpp:228] Iteration 5600, loss = 0.000442008
I0625 21:00:36.721912 12648 solver.cpp:244]     Train net output #0: loss = 0.000442041 (* 1 = 0.000442041 loss)
I0625 21:00:36.721912 12648 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I0625 21:00:42.128744 12648 solver.cpp:228] Iteration 5700, loss = 0.00425752
I0625 21:00:42.128744 12648 solver.cpp:244]     Train net output #0: loss = 0.00425757 (* 1 = 0.00425757 loss)
I0625 21:00:42.128744 12648 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I0625 21:00:47.410562 12648 solver.cpp:228] Iteration 5800, loss = 0.0281388
I0625 21:00:47.410562 12648 solver.cpp:244]     Train net output #0: loss = 0.0281389 (* 1 = 0.0281389 loss)
I0625 21:00:47.410562 12648 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I0625 21:00:52.973661 12648 solver.cpp:228] Iteration 5900, loss = 0.00373699
I0625 21:00:52.973661 12648 solver.cpp:244]     Train net output #0: loss = 0.00373705 (* 1 = 0.00373705 loss)
I0625 21:00:52.973661 12648 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I0625 21:00:58.099212 12648 solver.cpp:337] Iteration 6000, Testing net (#0)
I0625 21:01:01.552708 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9903
I0625 21:01:01.552708 12648 solver.cpp:404]     Test net output #1: loss = 0.027592 (* 1 = 0.027592 loss)
I0625 21:01:01.615216 12648 solver.cpp:228] Iteration 6000, loss = 0.00481358
I0625 21:01:01.615216 12648 solver.cpp:244]     Train net output #0: loss = 0.00481364 (* 1 = 0.00481364 loss)
I0625 21:01:01.615216 12648 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0625 21:01:06.787648 12648 solver.cpp:228] Iteration 6100, loss = 0.00309582
I0625 21:01:06.787648 12648 solver.cpp:244]     Train net output #0: loss = 0.00309588 (* 1 = 0.00309588 loss)
I0625 21:01:06.787648 12648 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I0625 21:01:11.960079 12648 solver.cpp:228] Iteration 6200, loss = 0.00738051
I0625 21:01:11.960079 12648 solver.cpp:244]     Train net output #0: loss = 0.00738057 (* 1 = 0.00738057 loss)
I0625 21:01:11.960079 12648 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I0625 21:01:17.116884 12648 solver.cpp:228] Iteration 6300, loss = 0.00991898
I0625 21:01:17.116884 12648 solver.cpp:244]     Train net output #0: loss = 0.00991904 (* 1 = 0.00991904 loss)
I0625 21:01:17.132511 12648 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I0625 21:01:22.304944 12648 solver.cpp:228] Iteration 6400, loss = 0.00873788
I0625 21:01:22.304944 12648 solver.cpp:244]     Train net output #0: loss = 0.00873793 (* 1 = 0.00873793 loss)
I0625 21:01:22.304944 12648 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I0625 21:01:27.539880 12648 solver.cpp:337] Iteration 6500, Testing net (#0)
I0625 21:01:30.977751 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9901
I0625 21:01:30.977751 12648 solver.cpp:404]     Test net output #1: loss = 0.0288114 (* 1 = 0.0288114 loss)
I0625 21:01:31.024631 12648 solver.cpp:228] Iteration 6500, loss = 0.0138209
I0625 21:01:31.024631 12648 solver.cpp:244]     Train net output #0: loss = 0.0138209 (* 1 = 0.0138209 loss)
I0625 21:01:31.024631 12648 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I0625 21:01:36.259568 12648 solver.cpp:228] Iteration 6600, loss = 0.0212077
I0625 21:01:36.259568 12648 solver.cpp:244]     Train net output #0: loss = 0.0212077 (* 1 = 0.0212077 loss)
I0625 21:01:36.259568 12648 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I0625 21:01:41.447628 12648 solver.cpp:228] Iteration 6700, loss = 0.00962339
I0625 21:01:41.447628 12648 solver.cpp:244]     Train net output #0: loss = 0.00962344 (* 1 = 0.00962344 loss)
I0625 21:01:41.447628 12648 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I0625 21:01:46.604431 12648 solver.cpp:228] Iteration 6800, loss = 0.00523888
I0625 21:01:46.604431 12648 solver.cpp:244]     Train net output #0: loss = 0.00523893 (* 1 = 0.00523893 loss)
I0625 21:01:46.604431 12648 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I0625 21:01:51.776863 12648 solver.cpp:228] Iteration 6900, loss = 0.00498403
I0625 21:01:51.776863 12648 solver.cpp:244]     Train net output #0: loss = 0.00498407 (* 1 = 0.00498407 loss)
I0625 21:01:51.776863 12648 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I0625 21:01:56.902415 12648 solver.cpp:337] Iteration 7000, Testing net (#0)
I0625 21:02:00.324657 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9904
I0625 21:02:00.340286 12648 solver.cpp:404]     Test net output #1: loss = 0.0282645 (* 1 = 0.0282645 loss)
I0625 21:02:00.387164 12648 solver.cpp:228] Iteration 7000, loss = 0.00674348
I0625 21:02:00.387164 12648 solver.cpp:244]     Train net output #0: loss = 0.00674353 (* 1 = 0.00674353 loss)
I0625 21:02:00.387164 12648 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0625 21:02:05.559597 12648 solver.cpp:228] Iteration 7100, loss = 0.0100275
I0625 21:02:05.559597 12648 solver.cpp:244]     Train net output #0: loss = 0.0100275 (* 1 = 0.0100275 loss)
I0625 21:02:05.559597 12648 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I0625 21:02:10.732028 12648 solver.cpp:228] Iteration 7200, loss = 0.00166741
I0625 21:02:10.732028 12648 solver.cpp:244]     Train net output #0: loss = 0.00166746 (* 1 = 0.00166746 loss)
I0625 21:02:10.732028 12648 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I0625 21:02:15.888833 12648 solver.cpp:228] Iteration 7300, loss = 0.0128539
I0625 21:02:15.888833 12648 solver.cpp:244]     Train net output #0: loss = 0.0128539 (* 1 = 0.0128539 loss)
I0625 21:02:15.888833 12648 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I0625 21:02:21.108144 12648 solver.cpp:228] Iteration 7400, loss = 0.00599509
I0625 21:02:21.108144 12648 solver.cpp:244]     Train net output #0: loss = 0.00599514 (* 1 = 0.00599514 loss)
I0625 21:02:21.108144 12648 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I0625 21:02:26.249322 12648 solver.cpp:337] Iteration 7500, Testing net (#0)
I0625 21:02:29.687192 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9899
I0625 21:02:29.687192 12648 solver.cpp:404]     Test net output #1: loss = 0.0307852 (* 1 = 0.0307852 loss)
I0625 21:02:29.749699 12648 solver.cpp:228] Iteration 7500, loss = 0.00090076
I0625 21:02:29.749699 12648 solver.cpp:244]     Train net output #0: loss = 0.000900808 (* 1 = 0.000900808 loss)
I0625 21:02:29.749699 12648 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I0625 21:02:34.937760 12648 solver.cpp:228] Iteration 7600, loss = 0.00554835
I0625 21:02:34.937760 12648 solver.cpp:244]     Train net output #0: loss = 0.0055484 (* 1 = 0.0055484 loss)
I0625 21:02:34.937760 12648 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I0625 21:02:40.094561 12648 solver.cpp:228] Iteration 7700, loss = 0.0248195
I0625 21:02:40.094561 12648 solver.cpp:244]     Train net output #0: loss = 0.0248195 (* 1 = 0.0248195 loss)
I0625 21:02:40.094561 12648 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I0625 21:02:45.298247 12648 solver.cpp:228] Iteration 7800, loss = 0.00243216
I0625 21:02:45.298247 12648 solver.cpp:244]     Train net output #0: loss = 0.00243222 (* 1 = 0.00243222 loss)
I0625 21:02:45.298247 12648 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I0625 21:02:50.470679 12648 solver.cpp:228] Iteration 7900, loss = 0.00578276
I0625 21:02:50.470679 12648 solver.cpp:244]     Train net output #0: loss = 0.00578281 (* 1 = 0.00578281 loss)
I0625 21:02:50.470679 12648 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I0625 21:02:55.611856 12648 solver.cpp:337] Iteration 8000, Testing net (#0)
I0625 21:02:59.065353 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9907
I0625 21:02:59.065353 12648 solver.cpp:404]     Test net output #1: loss = 0.0278031 (* 1 = 0.0278031 loss)
I0625 21:02:59.112233 12648 solver.cpp:228] Iteration 8000, loss = 0.00615647
I0625 21:02:59.112233 12648 solver.cpp:244]     Train net output #0: loss = 0.00615652 (* 1 = 0.00615652 loss)
I0625 21:02:59.112233 12648 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0625 21:03:04.269038 12648 solver.cpp:228] Iteration 8100, loss = 0.0216584
I0625 21:03:04.269038 12648 solver.cpp:244]     Train net output #0: loss = 0.0216585 (* 1 = 0.0216585 loss)
I0625 21:03:04.269038 12648 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I0625 21:03:09.457096 12648 solver.cpp:228] Iteration 8200, loss = 0.00508687
I0625 21:03:09.457096 12648 solver.cpp:244]     Train net output #0: loss = 0.00508692 (* 1 = 0.00508692 loss)
I0625 21:03:09.457096 12648 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I0625 21:03:14.645154 12648 solver.cpp:228] Iteration 8300, loss = 0.031042
I0625 21:03:14.645154 12648 solver.cpp:244]     Train net output #0: loss = 0.0310421 (* 1 = 0.0310421 loss)
I0625 21:03:14.645154 12648 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635568
I0625 21:03:19.817586 12648 solver.cpp:228] Iteration 8400, loss = 0.00924554
I0625 21:03:19.817586 12648 solver.cpp:244]     Train net output #0: loss = 0.00924558 (* 1 = 0.00924558 loss)
I0625 21:03:19.817586 12648 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I0625 21:03:24.943140 12648 solver.cpp:337] Iteration 8500, Testing net (#0)
I0625 21:03:28.381007 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9909
I0625 21:03:28.381007 12648 solver.cpp:404]     Test net output #1: loss = 0.028043 (* 1 = 0.028043 loss)
I0625 21:03:28.427887 12648 solver.cpp:228] Iteration 8500, loss = 0.00619204
I0625 21:03:28.427887 12648 solver.cpp:244]     Train net output #0: loss = 0.00619209 (* 1 = 0.00619209 loss)
I0625 21:03:28.427887 12648 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I0625 21:03:33.694079 12648 solver.cpp:228] Iteration 8600, loss = 0.00053192
I0625 21:03:33.694079 12648 solver.cpp:244]     Train net output #0: loss = 0.000531967 (* 1 = 0.000531967 loss)
I0625 21:03:33.694079 12648 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I0625 21:03:38.866511 12648 solver.cpp:228] Iteration 8700, loss = 0.00443238
I0625 21:03:38.866511 12648 solver.cpp:244]     Train net output #0: loss = 0.00443242 (* 1 = 0.00443242 loss)
I0625 21:03:38.866511 12648 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I0625 21:03:44.023316 12648 solver.cpp:228] Iteration 8800, loss = 0.0015269
I0625 21:03:44.023316 12648 solver.cpp:244]     Train net output #0: loss = 0.00152694 (* 1 = 0.00152694 loss)
I0625 21:03:44.023316 12648 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I0625 21:03:49.211374 12648 solver.cpp:228] Iteration 8900, loss = 0.00132382
I0625 21:03:49.211374 12648 solver.cpp:244]     Train net output #0: loss = 0.00132386 (* 1 = 0.00132386 loss)
I0625 21:03:49.211374 12648 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I0625 21:03:54.321300 12648 solver.cpp:337] Iteration 9000, Testing net (#0)
I0625 21:03:57.743541 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9904
I0625 21:03:57.743541 12648 solver.cpp:404]     Test net output #1: loss = 0.027493 (* 1 = 0.027493 loss)
I0625 21:03:57.806048 12648 solver.cpp:228] Iteration 9000, loss = 0.0142424
I0625 21:03:57.806048 12648 solver.cpp:244]     Train net output #0: loss = 0.0142425 (* 1 = 0.0142425 loss)
I0625 21:03:57.806048 12648 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0625 21:04:03.025362 12648 solver.cpp:228] Iteration 9100, loss = 0.0080495
I0625 21:04:03.025362 12648 solver.cpp:244]     Train net output #0: loss = 0.00804955 (* 1 = 0.00804955 loss)
I0625 21:04:03.025362 12648 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I0625 21:04:08.197793 12648 solver.cpp:228] Iteration 9200, loss = 0.00275031
I0625 21:04:08.197793 12648 solver.cpp:244]     Train net output #0: loss = 0.00275035 (* 1 = 0.00275035 loss)
I0625 21:04:08.197793 12648 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I0625 21:04:13.370224 12648 solver.cpp:228] Iteration 9300, loss = 0.00761587
I0625 21:04:13.370224 12648 solver.cpp:244]     Train net output #0: loss = 0.0076159 (* 1 = 0.0076159 loss)
I0625 21:04:13.370224 12648 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I0625 21:04:18.511402 12648 solver.cpp:228] Iteration 9400, loss = 0.052508
I0625 21:04:18.511402 12648 solver.cpp:244]     Train net output #0: loss = 0.052508 (* 1 = 0.052508 loss)
I0625 21:04:18.511402 12648 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I0625 21:04:23.636953 12648 solver.cpp:337] Iteration 9500, Testing net (#0)
I0625 21:04:27.074822 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9897
I0625 21:04:27.074822 12648 solver.cpp:404]     Test net output #1: loss = 0.0318979 (* 1 = 0.0318979 loss)
I0625 21:04:27.137329 12648 solver.cpp:228] Iteration 9500, loss = 0.0036209
I0625 21:04:27.137329 12648 solver.cpp:244]     Train net output #0: loss = 0.00362094 (* 1 = 0.00362094 loss)
I0625 21:04:27.137329 12648 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I0625 21:04:32.294134 12648 solver.cpp:228] Iteration 9600, loss = 0.00181629
I0625 21:04:32.294134 12648 solver.cpp:244]     Train net output #0: loss = 0.00181633 (* 1 = 0.00181633 loss)
I0625 21:04:32.294134 12648 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I0625 21:04:37.466567 12648 solver.cpp:228] Iteration 9700, loss = 0.0030865
I0625 21:04:37.466567 12648 solver.cpp:244]     Train net output #0: loss = 0.00308653 (* 1 = 0.00308653 loss)
I0625 21:04:37.466567 12648 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I0625 21:04:42.717131 12648 solver.cpp:228] Iteration 9800, loss = 0.0118926
I0625 21:04:42.717131 12648 solver.cpp:244]     Train net output #0: loss = 0.0118926 (* 1 = 0.0118926 loss)
I0625 21:04:42.717131 12648 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I0625 21:04:47.873936 12648 solver.cpp:228] Iteration 9900, loss = 0.0071501
I0625 21:04:47.873936 12648 solver.cpp:244]     Train net output #0: loss = 0.00715014 (* 1 = 0.00715014 loss)
I0625 21:04:47.873936 12648 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I0625 21:04:52.968235 12648 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0625 21:04:52.983861 12648 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0625 21:04:53.030742 12648 solver.cpp:317] Iteration 10000, loss = 0.00205264
I0625 21:04:53.030742 12648 solver.cpp:337] Iteration 10000, Testing net (#0)
I0625 21:04:56.468611 12648 solver.cpp:404]     Test net output #0: accuracy = 0.9913
I0625 21:04:56.468611 12648 solver.cpp:404]     Test net output #1: loss = 0.0269566 (* 1 = 0.0269566 loss)
I0625 21:04:56.468611 12648 solver.cpp:322] Optimization Done.
I0625 21:04:56.468611 12648 caffe.cpp:223] Optimization Done.
